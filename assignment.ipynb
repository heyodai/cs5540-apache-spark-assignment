{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5540, Group 1 - Apache Spark Assignment\n",
    "\n",
    "This is the submission document for our programming assignment over Apache Spark. \n",
    "\n",
    "The submission was written as a Jupyter notebook but will be exported to a PDF for submission. We can provide the GitHub repo or the original Jupyter notebook if requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in ./env/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in ./env/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col, split, explode, lower, trim, avg, expr, regexp_replace, struct\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"my-spark-app\")\\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\\\n",
    "    .getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Team Members](#team-members)\n",
    "2. [Question 1](#question-1)\n",
    "    - [1.1](#question-11)\n",
    "    - [1.2](#question-12)\n",
    "    - [1.3](#question-13)\n",
    "    - [1.4](#question-14)\n",
    "    - [1.5](#question-15)\n",
    "3. [Question 2](#question-2)\n",
    "    - [2.1](#question-21)\n",
    "    - [2.2](#question-22)\n",
    "    - [2.3](#question-23)\n",
    "4. [Question 3](#question-3)\n",
    "    - [3.1](#question-31)\n",
    "    - [3.2](#question-32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    "This assignment was completed by the following team members (Group 1):\n",
    "\n",
    "- Odai Athamneh\n",
    "- Scott Brunton\n",
    "- Ayushman (Jeet) Das\n",
    "- Koti Paruchuri\n",
    "- Varshith Thota"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Question 1 is as follows: \n",
    "\n",
    "> Given file (`/data/shakespeare-1.txt`) contains the scenes from Shakespeareâ€™s plays. You may use this \n",
    "file as an input dataset to identify the following notes for a student of Classical Drama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|word |\n",
      "+-----+\n",
      "|this |\n",
      "|is   |\n",
      "|the  |\n",
      "|100th|\n",
      "|etext|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize input text file and clean the word column\n",
    "df = spark.read.text(\"data/shakespeare-1.txt\")\n",
    "\n",
    "df = df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
    "df = df.select(lower(trim(col(\"word\"))).alias(\"word\"))\n",
    "\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> How many different countries are mentioned in the whole file? (Regardless of how many times a single country is mentioned, this country only contributes as a single entry). \n",
    "\n",
    "To address this question, we need a dataset of country names. We are using the `country-list.csv` file provided by the professor. The file contains 211 entries. \n",
    "\n",
    "The caveat to this approach is that the dataset may not contain all countries, such as: \n",
    "- Countries that no longer exist\n",
    "- Countries that are misspelled in the original Shakespearean text\n",
    "- Countries where the name or spelling has changed over time\n",
    "\n",
    "Addressing this issue is beyond the scope of this assignment and would likely require some degree of manual curation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|country       |\n",
      "+--------------+\n",
      "|afghanistan   |\n",
      "|albania       |\n",
      "|algeria       |\n",
      "|american samoa|\n",
      "|andorra       |\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load countries dataframe and clean the country column\n",
    "countries = spark.read.csv(\"data/country-list.csv\", header=False)\n",
    "countries = countries.select(lower(trim(\"_c0\")).alias(\"country\"))\n",
    "\n",
    "countries.show(5, truncate=False)\n",
    "countries.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our list of countries, we can use a simple `.join()` to find the number of countries mentioned in the Shakespearean text. We will use the `Country` column as our key and perform an inner join with the Shakespearean text. This will return a new DataFrame with only the rows that have a match in both DataFrames. We can then use `.count()` to get the number of rows in the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|country|\n",
      "+-------+\n",
      "|greece |\n",
      "|poland |\n",
      "|austria|\n",
      "|guinea |\n",
      "|france |\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Number of unique countries in the text file: 22\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "countries.createOrReplaceTempView(\"countries\")\n",
    "\n",
    "unique_countries = spark.sql(\"SELECT DISTINCT country FROM df JOIN countries ON df.word = countries.country\")\n",
    "unique_countries.show(5, truncate=False)\n",
    "\n",
    "print(\"Number of unique countries in the text file: {}\".format(unique_countries.count()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Compute the total number of times any country is mentioned. (This is different from  the  question1.1,  since  in  this  calculation,  if  a  country  is  mentioned  three  times,  then  it contributes three times). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|       392|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_mentions = spark.sql(\"SELECT country, COUNT(country) AS count FROM df JOIN countries ON df.word = countries.country GROUP BY country ORDER BY count DESC\")\n",
    "\n",
    "# show the sum of the count column\n",
    "country_mentions.agg({\"count\": \"sum\"}).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Determine  the  most  popular  countries.  (It  can  be  done  by  finding  the  three countries mentioned the most). \n",
    "\n",
    "This is fairly straightforward, and we can reuse the `country_mentions` variable from the last question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| country|count|\n",
      "+--------+-----+\n",
      "|  france|  149|\n",
      "| england|  128|\n",
      "|scotland|   24|\n",
      "+--------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_mentions.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> After exploring the dataset, now  calculate how many times specific countries are mentioned. (For example, how many times was France mentioned?) \n",
    "\n",
    "The code to do this is below and reuses the `country_mentions` variable again. Note that, by default, a Jupyter notebook will only show the first 20 rows of the resulting DataFrame. We use `.show(1000)` to ensure all rows are listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  country|count|\n",
      "+---------+-----+\n",
      "|   france|  149|\n",
      "|  england|  128|\n",
      "| scotland|   24|\n",
      "|    egypt|   15|\n",
      "|    wales|   15|\n",
      "|    italy|   12|\n",
      "|   cyprus|   10|\n",
      "|  denmark|   10|\n",
      "|   greece|    6|\n",
      "|     oman|    4|\n",
      "|   norway|    3|\n",
      "|  austria|    3|\n",
      "|    syria|    3|\n",
      "|    spain|    2|\n",
      "|   poland|    1|\n",
      "|   guinea|    1|\n",
      "|  iceland|    1|\n",
      "|  germany|    1|\n",
      "|palestine|    1|\n",
      "|   turkey|    1|\n",
      "|   russia|    1|\n",
      "|  armenia|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_mentions.show(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Finally, what is the average number of times a country is mentioned? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of countries mentioned in the text file: 53.83%\n",
      "Percentage of words in the text file that are countries: 0.03%\n"
     ]
    }
   ],
   "source": [
    "total_countries = countries.count()\n",
    "total_mentions = country_mentions.agg({\"count\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "avg_mentions = (total_countries/total_mentions) * 100\n",
    "avg_words = (total_mentions/df.count()) * 100\n",
    "\n",
    "print(\"Percentage of countries mentioned in the text file: {}%\".format(round(avg_mentions, 2)))\n",
    "print(\"Percentage of words in the text file that are countries: {}%\".format(round(avg_words, 2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "We must first define the schema. The schema must be defined and created in this case (rather then let Spark infer as is generally preferred) because there are additonal columns needed for a handful of rows. If we don't manually define the schema here, these rows would be rejected when Spark attempts to infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-------+-------+-------+-------+--------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|            _c0|    _c1|    _c2|    _c3|    _c4|    _c5|     _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|\n",
      "+---------------+-------+-------+-------+-------+-------+--------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|1419408000\\t0R1|Dn=038D|Dm=079D|Dx=120D|Sn=2.8M|Sm=6.0M| Sx=8.1M|null|null|null|null|null|null|null|null|null|null|null|\n",
      "|1419408001\\t0R1|Dn=038D|Dm=074D|Dx=120D|Sn=2.8M|Sm=6.2M| Sx=8.8M|null|null|null|null|null|null|null|null|null|null|null|\n",
      "|1419408002\\t0R1|Dn=038D|Dm=071D|Dx=120D|Sn=2.8M|Sm=6.5M| Sx=9.2M|null|null|null|null|null|null|null|null|null|null|null|\n",
      "|1419408003\\t0R1|Dn=038D|Dm=067D|Dx=120D|Sn=2.8M|Sm=6.8M| Sx=9.5M|null|null|null|null|null|null|null|null|null|null|null|\n",
      "|1419408004\\t0R1|Dn=038D|Dm=062D|Dx=081D|Sn=2.8M|Sm=7.2M|Sx=10.0M|null|null|null|null|null|null|null|null|null|null|null|\n",
      "+---------------+-------+-------+-------+-------+-------+--------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ \n",
    "    StructField(\"_c0\",StringType(),True), \\\n",
    "    StructField(\"_c1\",StringType(),True), \\\n",
    "    StructField(\"_c2\",StringType(),True), \\\n",
    "    StructField(\"_c3\",StringType(),True), \\\n",
    "    StructField(\"_c4\",StringType(),True), \\\n",
    "    StructField(\"_c5\",StringType(),True), \\\n",
    "    StructField(\"_c6\",StringType(),True), \\\n",
    "    StructField(\"_c7\",StringType(),True), \\\n",
    "    StructField(\"_c8\",StringType(),True), \\\n",
    "    StructField(\"_c9\",StringType(),True), \\\n",
    "    StructField(\"_c10\",StringType(),True), \\\n",
    "    StructField(\"_c11\",StringType(),True), \\\n",
    "    StructField(\"_c12\",StringType(),True), \\\n",
    "    StructField(\"_c13\",StringType(),True), \\\n",
    "    StructField(\"_c14\",StringType(),True), \\\n",
    "    StructField(\"_c15\",StringType(),True), \\\n",
    "    StructField(\"_c16\",StringType(),True), \\\n",
    "    StructField(\"_c17\",StringType(),True), \\\n",
    "  ])\n",
    "\n",
    "df = spark.read.schema(schema).csv('data/wx-data-1.txt', header=False)\n",
    "df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Find out how many days, weather was in ideal conditions? (Where Ideal condition means 8.5 â‰¤ Sm â‰¤ 9.0 M and 060D Dm â‰¤ 065D )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a staging df.  In staging df split the ID from the category reading type(Readtype).  Drop unsplit column before providing results to new df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStaging = df.withColumn('Id', split(df['_c0'], \"\\t\").getItem(0)).withColumn('ReadType', split(df['_c0'], '\\t').getItem(1)).drop(\"_c0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the data and create a df of only 0R1 type readings. Split the Measurement type from the value of the measurement.  Measurement type becomes column header.  Measurement value and measurement unit symbol are added to temp df.  Remove measurement unit symbol, leaving only measurement value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+---+---+---+----+\n",
      "|        Id| Dn| Dm| Dx| Sn| Sm|  Sx|\n",
      "+----------+---+---+---+---+---+----+\n",
      "|1419408000|038|079|120|2.8|6.0| 8.1|\n",
      "|1419408001|038|074|120|2.8|6.2| 8.8|\n",
      "|1419408002|038|071|120|2.8|6.5| 9.2|\n",
      "|1419408003|038|067|120|2.8|6.8| 9.5|\n",
      "|1419408004|038|062|081|2.8|7.2|10.0|\n",
      "+----------+---+---+---+---+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTemp1 = dfStaging.where(dfStaging.ReadType == '0R1').withColumn(\n",
    "    'Dn', split(dfStaging['_c1'], \"=\").getItem(1)).withColumn(\n",
    "        'Dm', split(dfStaging['_c2'], \"=\").getItem(1)).withColumn(\n",
    "            'Dx', split(dfStaging['_c3'], \"=\").getItem(1)).withColumn(\n",
    "                'Sn', split(dfStaging['_c4'], \"=\").getItem(1)).withColumn(\n",
    "                    'Sm', split(dfStaging['_c5'], \"=\").getItem(1)).withColumn(\n",
    "                        'Sx', split(dfStaging['_c6'], \"=\").getItem(1))         \n",
    "\n",
    "df0R1 = dfTemp1.select(dfTemp1.Id, regexp_replace(dfTemp1.Dn, \"D\", \"\").alias('Dn'),regexp_replace(\n",
    "    dfTemp1.Dm, \"D\", \"\").alias('Dm'), regexp_replace(\n",
    "        dfTemp1.Dx, \"D\", \"\").alias('Dx'), regexp_replace(\n",
    "            dfTemp1.Sn, \"M\", \"\").alias('Sn'), regexp_replace(\n",
    "                dfTemp1.Sm, \"M\", \"\").alias('Sm'), regexp_replace(\n",
    "                    dfTemp1.Sx, \"M\", \"\").alias('Sx'))\n",
    "\n",
    "df0R1.show(5) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the data and create a df of only 0R2 type readings. Split the Measurement type from the value of the measurement.  Measurement type becomes column header.  Measurement value and measurement unit symbol are added to temp df.  Remove measurement unit symbol, leaving only measurement value. \n",
    "\n",
    "0R2,Ta=14.4C,Ua=26.6P,Pa=889.6H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----+\n",
      "|        Id|  Ta|  Ua|   Pa|\n",
      "+----------+----+----+-----+\n",
      "|1419408006|13.9|28.5|889.9|\n",
      "|1419408016|13.9|28.5|889.9|\n",
      "|1419408026|13.9|28.4|889.9|\n",
      "|1419408036|13.9|28.3|889.7|\n",
      "|1419408046|13.9|28.3|889.9|\n",
      "+----------+----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTemp2 = dfStaging.where(dfStaging.ReadType == '0R2').withColumn(\n",
    "    'Ta', split(dfStaging['_c1'], \"=\").getItem(1)).withColumn(\n",
    "        'Ua', split(dfStaging['_c2'], \"=\").getItem(1)).withColumn(\n",
    "            'Pa', split(dfStaging['_c3'], \"=\").getItem(1))\n",
    "\n",
    "df0R2 = dfTemp2.select(dfTemp2.Id, regexp_replace(dfTemp2.Ta, \"C\", \"\").alias('Ta'),regexp_replace(\n",
    "    dfTemp2.Ua, \"P\", \"\").alias('Ua'), regexp_replace(\n",
    "        dfTemp2.Pa, \"H\", \"\").alias('Pa') )\n",
    "\n",
    "                \n",
    "df0R2.show(5)      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the data and create a df of only 0R5 type readings. Split the Measurement type from the value of the measurement.  Measurement type becomes column header.  Measurement value and measurement unit symbol are added to temp df.  Remove measurement unit symbol, leaving only measurement value. \n",
    "\n",
    "0R5,Th=13.3C,Vh=0.0#,Vs=25.6V,Vr=3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---+----+-----+\n",
      "|        Id|  Th| Vh|  Vs|   Vr|\n",
      "+----------+----+---+----+-----+\n",
      "|1419408023|13.1|0.0|25.6|3.516|\n",
      "|1419408083|13.1|0.0|25.5|3.518|\n",
      "|1419408143|12.9|0.0|25.6|3.516|\n",
      "|1419408203|13.3|0.0|25.6|3.516|\n",
      "|1419408263|13.1|0.0|25.6|3.518|\n",
      "+----------+----+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTemp3 = dfStaging.where(dfStaging.ReadType == '0R5').withColumn(\n",
    "    'Th', split(dfStaging['_c1'], \"=\").getItem(1)).withColumn(\n",
    "        'Vh', split(dfStaging['_c2'], \"=\").getItem(1)).withColumn(\n",
    "            'Vs', split(dfStaging['_c3'], \"=\").getItem(1)).withColumn(\n",
    "                'Vr', split(dfStaging['_c4'], \"=\").getItem(1))         \n",
    "\n",
    "df0R5 = dfTemp3.select(dfTemp3.Id, regexp_replace(dfTemp3.Th, \"C\", \"\").alias('Th'),regexp_replace(\n",
    "    dfTemp3.Vh, \"#\", \"\").alias('Vh'), regexp_replace(\n",
    "        dfTemp3.Vs, \"V\", \"\").alias('Vs'), regexp_replace(\n",
    "            dfTemp3.Vr, \"V\", \"\").alias('Vr'))\n",
    "\n",
    "df0R5.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the data and create a df of only 0R0 type readings. Split the Measurement type from the value of the measurement.  Measurement type becomes column header.  Measurement value and measurement unit symbol are added to temp df.  Remove measurement unit symbol, leaving only measurement value. \n",
    "\n",
    "Dn=058D,Dm=062D,Dx=064D,Sn=9.7M,Sm=10.3M,Sx=10.8M,Ta=14.4C,Ua=26.7P,Pa=889.6H,\n",
    "Rc=76.74M,Rd=34084s,Ri=0.0M,Hc=0.0M,Hd=0s,Hi=0.0M,Vs=25.5V,Vr=3.516V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+---+---+---+----+----+----+-----+-----+-----+---+---+---+---+----+-----+\n",
      "|        Id| Dn| Dm| Dx| Sn| Sm|  Sx|  Ta|  Ua|   Pa|   Rc|   Rd| Ri| Hc| Hd| Hi|  Vs|   Vr|\n",
      "+----------+---+---+---+---+---+----+----+----+-----+-----+-----+---+---+---+---+----+-----+\n",
      "|1419408024|057|064|069|8.8|9.6|10.3|13.9|28.5|889.9|76.74|34084|0.0|0.0|  0|0.0|25.6|3.516|\n",
      "|1419408084|056|062|066|8.4|8.8| 9.3|13.9|28.0|889.8|76.74|34084|0.0|0.0|  0|0.0|25.5|3.518|\n",
      "|1419408144|058|060|065|8.4|9.1|10.2|14.0|27.6|889.8|76.74|34084|0.0|0.0|  0|0.0|25.6|3.516|\n",
      "|1419408204|060|064|068|7.4|8.5| 8.9|14.2|27.8|889.9|76.74|34084|0.0|0.0|  0|0.0|25.6|3.516|\n",
      "|1419408264|056|057|060|7.5|8.1| 8.7|14.1|27.0|889.9|76.74|34084|0.0|0.0|  0|0.0|25.6|3.518|\n",
      "+----------+---+---+---+---+---+----+----+----+-----+-----+-----+---+---+---+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTemp4 = dfStaging.where(dfStaging.ReadType == '0R0').withColumn(\n",
    "    'Dn', split(dfStaging['_c1'], \"=\").getItem(1)).withColumn(\n",
    "        'Dm', split(dfStaging['_c2'], \"=\").getItem(1)).withColumn(\n",
    "            'Dx', split(dfStaging['_c3'], \"=\").getItem(1)).withColumn(\n",
    "                'Sn', split(dfStaging['_c4'], \"=\").getItem(1)).withColumn(\n",
    "                    'Sm', split(dfStaging['_c5'], \"=\").getItem(1)).withColumn(\n",
    "                        'Sx', split(dfStaging['_c6'], \"=\").getItem(1)).withColumn(\n",
    "    'Ta', split(dfStaging['_c7'], \"=\").getItem(1)).withColumn(\n",
    "        'Ua', split(dfStaging['_c8'], \"=\").getItem(1)).withColumn(\n",
    "            'Pa', split(dfStaging['_c9'], \"=\").getItem(1)).withColumn(\n",
    "                'Rc', split(dfStaging['_c10'], \"=\").getItem(1)).withColumn(\n",
    "                    'Rd', split(dfStaging['_c11'], \"=\").getItem(1)).withColumn(\n",
    "                        'Ri', split(dfStaging['_c12'], \"=\").getItem(1)).withColumn(\n",
    "    'Hc', split(dfStaging['_c13'], \"=\").getItem(1)).withColumn(\n",
    "        'Hd', split(dfStaging['_c14'], \"=\").getItem(1)).withColumn(\n",
    "            'Hi', split(dfStaging['_c15'], \"=\").getItem(1)).withColumn(\n",
    "                'Vs', split(dfStaging['_c16'], \"=\").getItem(1)).withColumn(\n",
    "                    'Vr', split(dfStaging['_c17'], \"=\").getItem(1))                              \n",
    "\n",
    "df0R0 = dfTemp4.select(dfTemp4.Id, regexp_replace(dfTemp4.Dn, \"D\", \"\").alias('Dn'),regexp_replace(\n",
    "    dfTemp4.Dm, \"D\", \"\").alias('Dm'), regexp_replace(\n",
    "        dfTemp4.Dx, \"D\", \"\").alias('Dx'), regexp_replace(\n",
    "            dfTemp4.Sn, \"M\", \"\").alias('Sn'), regexp_replace(\n",
    "                dfTemp4.Sm, \"M\", \"\").alias('Sm'), regexp_replace(\n",
    "                    dfTemp4.Sx, \"M\", \"\").alias('Sx'), regexp_replace(\n",
    "    dfTemp4.Ta, \"C\", \"\").alias('Ta'),regexp_replace(\n",
    "        dfTemp4.Ua, \"P\", \"\").alias('Ua'), regexp_replace(\n",
    "            dfTemp4.Pa, \"H\", \"\").alias('Pa'),  regexp_replace(\n",
    "                dfTemp4.Rc, \"M\", \"\").alias('Rc'),  regexp_replace(\n",
    "                    dfTemp4.Rd, \"s\", \"\").alias('Rd'),  regexp_replace(\n",
    "                        dfTemp4.Ri, \"M\", \"\").alias('Ri'),  regexp_replace(\n",
    "    dfTemp4.Hc, \"M\", \"\").alias('Hc'),  regexp_replace(\n",
    "        dfTemp4.Hd, \"s\", \"\").alias('Hd'),  regexp_replace(\n",
    "            dfTemp4.Hi, \"M\", \"\").alias('Hi'),  regexp_replace(\n",
    "                dfTemp4.Vs, \"V\", \"\").alias('Vs'),  regexp_replace(\n",
    "                    dfTemp4.Vr, \"V\", \"\").alias('Vr'))       \n",
    "\n",
    "df0R0.show(5) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to use distinct count of id for count of days.  Id will be used to join all read df as it is duplicated accross all readtypes.\n",
    "\n",
    "(Where Ideal condition means 8.5 â‰¤ Sm â‰¤ 9.0 M and 060D Dm â‰¤ 065D )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 15:49:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+---------+\n",
      "|IdealDays|\n",
      "+---------+\n",
      "|      182|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0R0.createOrReplaceTempView('0R0')\n",
    "df0R1.createOrReplaceTempView('0R1')\n",
    "df0R2.createOrReplaceTempView('0R2')\n",
    "df0R5.createOrReplaceTempView('0R5')\n",
    "\n",
    "spark.sql('select count(*) as IdealDays from 0R1 as a left join 0R0 as b on a.Id = b.Id ' +\n",
    "        ' left join 0R2 as c on a.Id = c.Id ' +\n",
    "        ' left join 0R5 as d on a.Id = d.Id ' +\n",
    "        ' where (a.Sm < 8.5 or b.Sm > 9.0) and ' +\n",
    "        ' (b.Sm < 8.5 or a.Sm > 9.0) and ' +\n",
    "        ' (a.Dm <= 065 or b.Dm <= 065)').show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Find out what the are minimum values for Sn and Dn?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, our cleanup work from the previous question makes this question very easy to answer. We can simply use `.min()` on the `Sn` and `Dn` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|MinDn|\n",
      "+-----+\n",
      "|  000|\n",
      "|  000|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|MinSn|\n",
      "+-----+\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select min(a.Dn) as MinDn from 0R1 as a union all select min(b.Dn) from 0R0 as b').show(10)\n",
    "spark.sql('select min(a.Sn) as MinSn from 0R1 as a union all select min(b.Sn) from 0R0 as b').show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Find out what the are maximum values for Sx are and Dx?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, so below. We can use `.max()` on the `Sx` and `Dx` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|MaxSx|\n",
      "+-----+\n",
      "|  9.9|\n",
      "|  9.9|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|MaxDx|\n",
      "+-----+\n",
      "|  359|\n",
      "|  359|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select max(a.Sx) as MaxSx from 0R1 as a union all select max(b.Sx) from 0R0 as b').show(10)\n",
    "spark.sql('select max(a.Dx) as MaxDx from 0R1 as a union all select max(b.Dx) from 0R0 as b').show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "|number|air_pressure.|  air_temp.|avg_wind_direction.|avg_wind_speed.|max_wind_direction.|max_wind_speed.|relative_humidity.|\n",
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "|     0|       918.06|     74.822|              271.1|      2.0803542|              295.4|      2.8632832|             42.42|\n",
      "|     1|  917.3476881|71.40384263|        101.9351794|    2.443009216|        140.4715485|    3.533323602|       24.32869729|\n",
      "|     2|       923.04|     60.638|                 51|     17.0678522|               63.7|     22.1009672|               8.9|\n",
      "|     3|  920.5027512|70.13889487|        198.8321327|    4.337363056|        211.2033412|     5.19004536|       12.18910187|\n",
      "|     4|       921.16|     44.294|              277.8|      1.8566602|              136.5|      2.8632832|             92.41|\n",
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/daily_weather-2.csv', header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------------+--------------+------------------+--------------+-----------------+\n",
      "|air_pressure|   air_temp|avg_wind_direction|avg_wind_speed|max_wind_direction|max_wind_speed|relative_humidity|\n",
      "+------------+-----------+------------------+--------------+------------------+--------------+-----------------+\n",
      "|      918.06|     74.822|             271.1|     2.0803542|             295.4|     2.8632832|            42.42|\n",
      "| 917.3476881|71.40384263|       101.9351794|   2.443009216|       140.4715485|   3.533323602|      24.32869729|\n",
      "|      923.04|     60.638|                51|    17.0678522|              63.7|    22.1009672|              8.9|\n",
      "| 920.5027512|70.13889487|       198.8321327|   4.337363056|       211.2033412|    5.19004536|      12.18910187|\n",
      "|      921.16|     44.294|             277.8|     1.8566602|             136.5|     2.8632832|            92.41|\n",
      "+------------+-----------+------------------+--------------+------------------+--------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_cols=(column.replace('.', '') for column in df.columns)\n",
    "df = df.toDF(*new_cols)\n",
    "df = df.drop(\"number\")\n",
    "df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Count the number of days where all parameters have difference of Â± 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally pivoting the column headers to the column typeMeasurement, these will be used later to create a relation to the DataTable.  Column name, avgs, lower and upper ranges will be collected to create a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+------------------+\n",
      "|index|   typeMeasurement|               avg|           upprRng|           lowrRng|\n",
      "+-----+------------------+------------------+------------------+------------------+\n",
      "|    0|                ab|               0.0|               0.0|               0.0|\n",
      "|    1|      air_pressure| 918.8825513141026| 920.8825513141026| 916.8825513141026|\n",
      "|    2|          air_temp| 64.93300141293575| 66.93300141293575| 62.93300141293575|\n",
      "|    3|avg_wind_direction|142.23551070020164|144.23551070020164|140.23551070020164|\n",
      "|    4|    avg_wind_speed| 5.508284242259157| 7.508284242259157|3.5082842422591574|\n",
      "+-----+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfAvg = df.select( avg('air_pressure'), avg('air_temp'), avg('avg_wind_direction'), avg('avg_wind_speed'),\n",
    "                           avg('max_wind_direction'), avg('max_wind_speed'), avg('relative_humidity'))\n",
    "#set to one to offset row used for intilization of list var data\n",
    "x = 1                   \n",
    "data = [(0,\"ab\",0.0,0.0,0.0)]\n",
    "\n",
    "for col in df.columns:\n",
    "  typOfMeasurment = x\n",
    "  avrg = dfAvg.collect()[0][x -1]\n",
    "  upprRng = avrg + 2\n",
    "  lowrRng = avrg -2\n",
    "\n",
    "  data.append([x,col,avrg,upprRng,lowrRng])\n",
    "  x += 1\n",
    "  \n",
    "dfAvgSummary = spark.createDataFrame(schema = ['index','typeMeasurement','avg','upprRng','lowrRng'], data = data)\n",
    "dfAvgSummary.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes record that was only used to intilize the column data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+------------------+\n",
      "|index|   typeMeasurement|               avg|           upprRng|           lowrRng|\n",
      "+-----+------------------+------------------+------------------+------------------+\n",
      "|    1|      air_pressure| 918.8825513141026| 920.8825513141026| 916.8825513141026|\n",
      "|    2|          air_temp| 64.93300141293575| 66.93300141293575| 62.93300141293575|\n",
      "|    3|avg_wind_direction|142.23551070020164|144.23551070020164|140.23551070020164|\n",
      "|    4|    avg_wind_speed| 5.508284242259157| 7.508284242259157|3.5082842422591574|\n",
      "|    5|max_wind_direction|148.95351796495402|150.95351796495402|146.95351796495402|\n",
      "+-----+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean DF and remove values that were used to initilize df columen types\n",
    "dfAvgSummaryC = dfAvgSummary.filter(dfAvgSummary.index != 0)\n",
    "dfAvgSummaryC.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Loop to iterate through each column in the datatable.  Once the column is selected the correlating lower and upper range are also retrived.  The lower and upper range are type cast to perform regular expression to only retreive the ranges(numeric values).  Column, lower and upper range are appended to query to display number of records outside of the +-2 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(air_pressure)|\n",
      "+-------------------+\n",
      "|                606|\n",
      "+-------------------+\n",
      "\n",
      "+---------------+\n",
      "|count(air_temp)|\n",
      "+---------------+\n",
      "|            951|\n",
      "+---------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(avg_wind_direction)|\n",
      "+-------------------------+\n",
      "|                     1084|\n",
      "+-------------------------+\n",
      "\n",
      "+---------------------+\n",
      "|count(avg_wind_speed)|\n",
      "+---------------------+\n",
      "|                  754|\n",
      "+---------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(max_wind_direction)|\n",
      "+-------------------------+\n",
      "|                     1082|\n",
      "+-------------------------+\n",
      "\n",
      "+---------------------+\n",
      "|count(max_wind_speed)|\n",
      "+---------------------+\n",
      "|                  826|\n",
      "+---------------------+\n",
      "\n",
      "+------------------------+\n",
      "|count(relative_humidity)|\n",
      "+------------------------+\n",
      "|                    1041|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('dataTable')\n",
    "dfAvgSummaryC.createOrReplaceTempView('avgTable')\n",
    "\n",
    "exp = r\"[^0-9.]\"\n",
    "\n",
    "dataTableCol = df.columns\n",
    "avgCols = dfAvg.columns\n",
    "\n",
    "for dCol in dataTableCol:\n",
    "  upprRng = dfAvgSummaryC.select(dfAvgSummaryC.upprRng).filter(dfAvgSummaryC.typeMeasurement.contains(dCol)).collect()  \n",
    "  lowrRng = dfAvgSummaryC.select(dfAvgSummaryC.lowrRng).filter(dfAvgSummaryC.typeMeasurement.contains(dCol)).collect()\n",
    "  \n",
    "  x =''.join(map(str, upprRng)) \n",
    "  upr = re.sub(exp, '', x)\n",
    "  y =''.join(map(str, lowrRng)) \n",
    "  lowr = re.sub(exp, '', y)\n",
    "  \n",
    "  spark.sql(\"select count(dt.\" + dCol + \") from dataTable as dt\" +\n",
    "                      \" where dt.\" + dCol + \" > \" + upr +\n",
    "                      \" or dt.\" + dCol + \" < \" + lowr ).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Count number of days where max_wind_speed and ang_wind_speed has difference more than 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "|number|air_pressure.|  air_temp.|avg_wind_direction.|avg_wind_speed.|max_wind_direction.|max_wind_speed.|relative_humidity.|\n",
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "|     0|       918.06|     74.822|              271.1|      2.0803542|              295.4|      2.8632832|             42.42|\n",
      "|     1|  917.3476881|71.40384263|        101.9351794|    2.443009216|        140.4715485|    3.533323602|       24.32869729|\n",
      "|     2|       923.04|     60.638|                 51|     17.0678522|               63.7|     22.1009672|               8.9|\n",
      "|     3|  920.5027512|70.13889487|        198.8321327|    4.337363056|        211.2033412|     5.19004536|       12.18910187|\n",
      "|     4|       921.16|     44.294|              277.8|      1.8566602|              136.5|      2.8632832|             92.41|\n",
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/daily_weather-2.csv', header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|5DaysGreater|\n",
      "+------------+\n",
      "|          20|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_cols=(column.replace('.', '') for column in df.columns)\n",
    "df = df.toDF(*new_cols)\n",
    "\n",
    "df.createOrReplaceTempView(\"dataT\")\n",
    "\n",
    "spark.sql(\"select  count(*) as 5DaysGreater \" +      \n",
    "          \" from dataT dt \" +\n",
    "          \" where dt.max_wind_speed - dt.avg_wind_speed  > 5\").show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52d158634e85a4a13156a2aa9aa4d203fab0b0eaa50959d6ad1d28e12f0e62a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
