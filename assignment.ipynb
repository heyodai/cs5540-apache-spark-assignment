{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5540, Group 1 - Apache Spark Assignment\n",
    "\n",
    "This is the submission document for our programming assignment over Apache Spark. \n",
    "\n",
    "The submission was written as a Jupyter notebook but will be exported to a PDF for submission. We can provide the GitHub repo or the original Jupyter notebook if requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in ./env/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in ./env/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col, split, explode, lower, trim, avg\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"my-spark-app\")\\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\\\n",
    "    .getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Team Members](#team-members)\n",
    "2. [Question 1](#question-1)\n",
    "    - [1.1](#11)\n",
    "    - [1.2](#12)\n",
    "    - [1.3](#13)\n",
    "    - [1.4](#14)\n",
    "    - [1.5](#15)\n",
    "3. [Question 2](#question-2)\n",
    "4. [Question 3](#question-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    "This assignment was completed by the following team members (Group 1):\n",
    "\n",
    "- Odai Athamneh\n",
    "- Scott Brunton\n",
    "- Ayushman (Jeet) Das\n",
    "- Koti Paruchuri\n",
    "- Varshith Thota"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Question 1 is as follows: \n",
    "\n",
    "> Given file (`/data/shakespeare-1.txt`) contains the scenes from Shakespeare’s plays. You may use this \n",
    "file as an input dataset to identify the following notes for a student of Classical Drama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|word |\n",
      "+-----+\n",
      "|this |\n",
      "|is   |\n",
      "|the  |\n",
      "|100th|\n",
      "|etext|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize input text file and clean the word column\n",
    "df = spark.read.text(\"data/shakespeare-1.txt\")\n",
    "\n",
    "df = df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
    "df = df.select(lower(trim(col(\"word\"))).alias(\"word\"))\n",
    "\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> How many different countries are mentioned in the whole file? (Regardless of how many times a single country is mentioned, this country only contributes as a single entry). \n",
    "\n",
    "To address this question, we need a dataset of country names. We are using the `country-list.csv` file provided by the professor. The file contains 211 entries. \n",
    "\n",
    "The caveat to this approach is that the dataset may not contain all countries, such as: \n",
    "- Countries that no longer exist\n",
    "- Countries that are misspelled in the original Shakespearean text\n",
    "- Countries where the name or spelling has changed over time\n",
    "\n",
    "Addressing this issue is beyond the scope of this assignment and would likely require some degree of manual curation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|country       |\n",
      "+--------------+\n",
      "|afghanistan   |\n",
      "|albania       |\n",
      "|algeria       |\n",
      "|american samoa|\n",
      "|andorra       |\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load countries dataframe and clean the country column\n",
    "countries = spark.read.csv(\"data/country-list.csv\", header=False)\n",
    "countries = countries.select(lower(trim(\"_c0\")).alias(\"country\"))\n",
    "\n",
    "countries.show(5, truncate=False)\n",
    "countries.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our list of countries, we can use a simple `.join()` to find the number of countries mentioned in the Shakespearean text. We will use the `Country` column as our key and perform an inner join with the Shakespearean text. This will return a new DataFrame with only the rows that have a match in both DataFrames. We can then use `.count()` to get the number of rows in the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|country|\n",
      "+-------+\n",
      "|greece |\n",
      "|poland |\n",
      "|austria|\n",
      "|guinea |\n",
      "|france |\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Number of unique countries in the text file: 22\n"
     ]
    }
   ],
   "source": [
    "# perform join after converting both columns to lowercase and trimming the country column\n",
    "unique_countries = df.join(countries, df.word == countries.country, \"inner\").select(\"country\").distinct()\n",
    "unique_countries.show(5, truncate=False)\n",
    "\n",
    "print(\"Number of unique countries in the text file: {}\".format(unique_countries.count()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Compute the total number of times any country is mentioned. (This is different from  the  question1.1,  since  in  this  calculation,  if  a  country  is  mentioned  three  times,  then  it contributes three times). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|       392|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_mentions = df.join(countries, df.word == countries.country, \"inner\").select(\"country\").groupBy(\"country\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# show the sum of the count column\n",
    "country_mentions.agg({\"count\": \"sum\"}).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Determine  the  most  popular  countries.  (It  can  be  done  by  finding  the  three countries mentioned the most). \n",
    "\n",
    "This is fairly straightforward, and we can reuse the `country_mentions` variable from the last question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| country|count|\n",
      "+--------+-----+\n",
      "|  france|  149|\n",
      "| england|  128|\n",
      "|scotland|   24|\n",
      "+--------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_mentions.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> After exploring the dataset, now  calculate how many times specific countries are mentioned. (For example, how many times was France mentioned?) \n",
    "\n",
    "The code to do this is below and reuses the `country_mentions` variable again. Note that, by default, a Jupyter notebook will only show the first 20 rows of the resulting DataFrame. We use `.show(1000)` to ensure all rows are listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  country|count|\n",
      "+---------+-----+\n",
      "|   france|  149|\n",
      "|  england|  128|\n",
      "| scotland|   24|\n",
      "|    egypt|   15|\n",
      "|    wales|   15|\n",
      "|    italy|   12|\n",
      "|   cyprus|   10|\n",
      "|  denmark|   10|\n",
      "|   greece|    6|\n",
      "|     oman|    4|\n",
      "|   norway|    3|\n",
      "|  austria|    3|\n",
      "|    syria|    3|\n",
      "|    spain|    2|\n",
      "|   poland|    1|\n",
      "|   guinea|    1|\n",
      "|  iceland|    1|\n",
      "|  germany|    1|\n",
      "|palestine|    1|\n",
      "|   turkey|    1|\n",
      "|   russia|    1|\n",
      "|  armenia|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_mentions.show(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Finally, what is the average number of times a country is mentioned? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of countries mentioned in the text file: 53.83%\n",
      "Percentage of words in the text file that are countries: 0.03%\n"
     ]
    }
   ],
   "source": [
    "total_countries = countries.count()\n",
    "total_mentions = country_mentions.agg({\"count\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "avg_mentions = (total_countries/total_mentions) * 100\n",
    "avg_words = (total_mentions/df.count()) * 100\n",
    "\n",
    "print(\"Percentage of countries mentioned in the text file: {}%\".format(round(avg_mentions, 2)))\n",
    "print(\"Percentage of words in the text file that are countries: {}%\".format(round(avg_words, 2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "|number|air_pressure.|  air_temp.|avg_wind_direction.|avg_wind_speed.|max_wind_direction.|max_wind_speed.|relative_humidity.|\n",
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "|     0|       918.06|     74.822|              271.1|      2.0803542|              295.4|      2.8632832|             42.42|\n",
      "|     1|  917.3476881|71.40384263|        101.9351794|    2.443009216|        140.4715485|    3.533323602|       24.32869729|\n",
      "|     2|       923.04|     60.638|                 51|     17.0678522|               63.7|     22.1009672|               8.9|\n",
      "|     3|  920.5027512|70.13889487|        198.8321327|    4.337363056|        211.2033412|     5.19004536|       12.18910187|\n",
      "|     4|       921.16|     44.294|              277.8|      1.8566602|              136.5|      2.8632832|             92.41|\n",
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/daily_weather-2.csv', header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------------+--------------+------------------+--------------+-----------------+\n",
      "|air_pressure|   air_temp|avg_wind_direction|avg_wind_speed|max_wind_direction|max_wind_speed|relative_humidity|\n",
      "+------------+-----------+------------------+--------------+------------------+--------------+-----------------+\n",
      "|      918.06|     74.822|             271.1|     2.0803542|             295.4|     2.8632832|            42.42|\n",
      "| 917.3476881|71.40384263|       101.9351794|   2.443009216|       140.4715485|   3.533323602|      24.32869729|\n",
      "|      923.04|     60.638|                51|    17.0678522|              63.7|    22.1009672|              8.9|\n",
      "| 920.5027512|70.13889487|       198.8321327|   4.337363056|       211.2033412|    5.19004536|      12.18910187|\n",
      "|      921.16|     44.294|             277.8|     1.8566602|             136.5|     2.8632832|            92.41|\n",
      "+------------+-----------+------------------+--------------+------------------+--------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_cols=(column.replace('.', '') for column in df.columns)\n",
    "df = df.toDF(*new_cols)\n",
    "df = df.drop(\"number\")\n",
    "df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Count the number of days where all parameters have difference of ± 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally pivoting the column headers to the column typeMeasurement, these will be used later to create a relation to the DataTable.  Column name, avgs, lower and upper ranges will be collected to create a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+------------------+\n",
      "|index|   typeMeasurement|               avg|           upprRng|           lowrRng|\n",
      "+-----+------------------+------------------+------------------+------------------+\n",
      "|    0|                ab|               0.0|               0.0|               0.0|\n",
      "|    1|      air_pressure| 918.8825513141026| 920.8825513141026| 916.8825513141026|\n",
      "|    2|          air_temp| 64.93300141293575| 66.93300141293575| 62.93300141293575|\n",
      "|    3|avg_wind_direction|142.23551070020164|144.23551070020164|140.23551070020164|\n",
      "|    4|    avg_wind_speed| 5.508284242259157| 7.508284242259157|3.5082842422591574|\n",
      "|    5|max_wind_direction|148.95351796495402|150.95351796495402|146.95351796495402|\n",
      "|    6|    max_wind_speed| 7.019513529173236| 9.019513529173235| 5.019513529173236|\n",
      "|    7| relative_humidity|34.241402059256586|36.241402059256586|32.241402059256586|\n",
      "+-----+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfAvg = df.select( avg('air_pressure'), avg('air_temp'), avg('avg_wind_direction'), avg('avg_wind_speed'),\n",
    "                           avg('max_wind_direction'), avg('max_wind_speed'), avg('relative_humidity'))\n",
    "#set to one to offset row used for intilization of list var data\n",
    "x = 1                   \n",
    "data = [(0,\"ab\",0.0,0.0,0.0)]\n",
    "\n",
    "for col in df.columns:\n",
    "  typOfMeasurment = x\n",
    "  avrg = dfAvg.collect()[0][x -1]\n",
    "  upprRng = avrg + 2\n",
    "  lowrRng = avrg -2\n",
    "\n",
    "  data.append([x,col,avrg,upprRng,lowrRng])\n",
    "  x += 1\n",
    "  \n",
    "dfAvgSummary = spark.createDataFrame(schema = ['index','typeMeasurement','avg','upprRng','lowrRng'], data = data)\n",
    "dfAvgSummary.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes record that was only used to intilize the column data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+------------------+\n",
      "|index|   typeMeasurement|               avg|           upprRng|           lowrRng|\n",
      "+-----+------------------+------------------+------------------+------------------+\n",
      "|    1|      air_pressure| 918.8825513141026| 920.8825513141026| 916.8825513141026|\n",
      "|    2|          air_temp| 64.93300141293575| 66.93300141293575| 62.93300141293575|\n",
      "|    3|avg_wind_direction|142.23551070020164|144.23551070020164|140.23551070020164|\n",
      "|    4|    avg_wind_speed| 5.508284242259157| 7.508284242259157|3.5082842422591574|\n",
      "|    5|max_wind_direction|148.95351796495402|150.95351796495402|146.95351796495402|\n",
      "|    6|    max_wind_speed| 7.019513529173236| 9.019513529173235| 5.019513529173236|\n",
      "|    7| relative_humidity|34.241402059256586|36.241402059256586|32.241402059256586|\n",
      "+-----+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean DF and remove values that were used to initilize df columen types\n",
    "dfAvgSummaryC = dfAvgSummary.filter(dfAvgSummary.index != 0)\n",
    "dfAvgSummaryC.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Loop to iterate through each column in the datatable.  Once the column is selected the correlating lower and upper range are also retrived.  The lower and upper range are type cast to perform regular expression to only retreive the ranges(numeric values).  Column, lower and upper range are appended to query to display number of records outside of the +-2 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(air_pressure)|\n",
      "+-------------------+\n",
      "|                606|\n",
      "+-------------------+\n",
      "\n",
      "+---------------+\n",
      "|count(air_temp)|\n",
      "+---------------+\n",
      "|            951|\n",
      "+---------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(avg_wind_direction)|\n",
      "+-------------------------+\n",
      "|                     1084|\n",
      "+-------------------------+\n",
      "\n",
      "+---------------------+\n",
      "|count(avg_wind_speed)|\n",
      "+---------------------+\n",
      "|                  754|\n",
      "+---------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(max_wind_direction)|\n",
      "+-------------------------+\n",
      "|                     1082|\n",
      "+-------------------------+\n",
      "\n",
      "+---------------------+\n",
      "|count(max_wind_speed)|\n",
      "+---------------------+\n",
      "|                  826|\n",
      "+---------------------+\n",
      "\n",
      "+------------------------+\n",
      "|count(relative_humidity)|\n",
      "+------------------------+\n",
      "|                    1041|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('dataTable')\n",
    "dfAvgSummaryC.createOrReplaceTempView('avgTable')\n",
    "\n",
    "exp = r\"[^0-9.]\"\n",
    "\n",
    "dataTableCol = df.columns\n",
    "avgCols = dfAvg.columns\n",
    "\n",
    "for dCol in dataTableCol:\n",
    "  upprRng = dfAvgSummaryC.select(dfAvgSummaryC.upprRng).filter(dfAvgSummaryC.typeMeasurement.contains(dCol)).collect()  \n",
    "  lowrRng = dfAvgSummaryC.select(dfAvgSummaryC.lowrRng).filter(dfAvgSummaryC.typeMeasurement.contains(dCol)).collect()\n",
    "  \n",
    "  x =''.join(map(str, upprRng)) \n",
    "  upr = re.sub(exp, '', x)\n",
    "  y =''.join(map(str, lowrRng)) \n",
    "  lowr = re.sub(exp, '', y)\n",
    "  \n",
    "  spark.sql(\"select count(dt.\" + dCol + \") from dataTable as dt\" +\n",
    "                      \" where dt.\" + dCol + \" > \" + upr +\n",
    "                      \" or dt.\" + dCol + \" < \" + lowr ).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "\n",
    "The question reads as follows:\n",
    "\n",
    "> Count number of days where max_wind_speed and ang_wind_speed has difference more than 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "|number|air_pressure.|  air_temp.|avg_wind_direction.|avg_wind_speed.|max_wind_direction.|max_wind_speed.|relative_humidity.|\n",
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "|     0|       918.06|     74.822|              271.1|      2.0803542|              295.4|      2.8632832|             42.42|\n",
      "|     1|  917.3476881|71.40384263|        101.9351794|    2.443009216|        140.4715485|    3.533323602|       24.32869729|\n",
      "|     2|       923.04|     60.638|                 51|     17.0678522|               63.7|     22.1009672|               8.9|\n",
      "|     3|  920.5027512|70.13889487|        198.8321327|    4.337363056|        211.2033412|     5.19004536|       12.18910187|\n",
      "|     4|       921.16|     44.294|              277.8|      1.8566602|              136.5|      2.8632832|             92.41|\n",
      "+------+-------------+-----------+-------------------+---------------+-------------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/daily_weather-2.csv', header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|5DaysGreater|\n",
      "+------------+\n",
      "|          20|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_cols=(column.replace('.', '') for column in df.columns)\n",
    "df = df.toDF(*new_cols)\n",
    "\n",
    "df.createOrReplaceTempView(\"dataT\")\n",
    "\n",
    "spark.sql(\"select  count(*) as 5DaysGreater \" +      \n",
    "          \" from dataT dt \" +\n",
    "          \" where dt.max_wind_speed - dt.avg_wind_speed  > 5\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52d158634e85a4a13156a2aa9aa4d203fab0b0eaa50959d6ad1d28e12f0e62a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
